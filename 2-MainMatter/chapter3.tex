%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{Work Plan}
\label{cha:work_plan}

%\graphicspath{{5-Figures/diagrams/}}

This chapter aims at defining the scope of our work, what our goal is, the plan to execute it and what has already been done, along with its preliminary results.


\section{Overview} % (fold)
\label{sec:work_overview}

At this stage, we have developed a data extraction tool to build the necessary foundation for such a system. This tool leverages the Protein Data Bank (PDB) to identify proteins with known 3D structures and then targets their associated scientific literature. We operate on the logic that a protein must be successfully purified before its structure can be experimentally determined, so these papers represent a reliable source of proven purification methodologies.

With this data, we plan to train a predictive model, using a Transformer-based architecture due to its strength in generating sequential instructions. This model will need to be part of a larger pipeline that manages data inputs and ensures the final output is technically consistent. These later stages remain theoretical and will be discussed further in the future work plan section.


\section{Work Done} % (fold)
\label{sec:work_done}

The development of a robust predictive model is fundamentally dependent on the quality and volume of the training data. For this project, the primary objective is to correlate the physico-chemical properties of a protein with its optimal purification strategy. Because no centralized database currently exists that maps these biochemical attributes to specific experimental protocols, a significant portion of the initial work focused on the design and implementation of an automated data mining pipeline.

The pipeline was engineered to identify, retrieve, and process scientific literature to build a structured dataset. This process was divided into four distinct phases:
\begin{enumerate}
    \item \textit{Source Identification:} Determining from where we could source protein purification protocols.
    \item \textit{Document Linking:} Mapping specific protein sequences and structures to the papers that describe their purification.
    \item \textit{Full-text Retrieval:} Automating the retrieval of the complete text of identified papers.
    \item \textit{Information Extraction:} Processing the unstructured text to isolate chromatography steps and related biochemical metadata.
\end{enumerate}

\subsection{Determining sources of purification protocol}

In order to get a high volume of quality training data, we would need a source of correctly documented protein purification protocols and the proteins they are being applied to. Given that there is no centralized database with such information, and selecting by hand would prove unfeasible, the first step was to find a solution to this problem. In the context of biochemistry, the most comprehensive descriptions of protein purification processes are found within peer-reviewed scientific papers. However, the vast volume of published literature demands a systematic method for identifying relevant documents, as manual selection is not scalable for a dataset of the size required for deep learning.

To solve this, I utilized the PDB\footnote{\url{https://www.rcsb.org/}} as the primary gateway for data discovery. Each entry in the PDB is typically associated with a primary citation, which is the scientific paper detailing the methods used to determine that specific structure.

The logical basis for this approach is that the determination of a protein's structure requires a highly purified sample. Consequently, the primary citation for a PDB entry almost universally includes a methodology section describing the purification protocol used to reach the required level of purity. At the time of this research, the PDB contained 247,417 entries. While a subset of these entries may lack an associated paper or detailed methodology, the sheer scale of the database provides a sufficient foundation for training a predictive model.

To extract this information programmatically, I interfaced with the PDB API. This allowed for the automated querying of specific metadata fields for every entry in the database. For the current phase, the relevant fields were PDB ID, UniProt ID and the associated paper's bibliographic metadata, such as the DOI, PubMed ID and title.

The UniProt ID is particularly important because it allows us to link the 3D structure in the PDB to the protein sequence it corresponds to. Not only that, since UniProt\footnote{\url{https://www.uniprot.org/}} is cross-referenced with 185 other databases, from genomics to biochemistry, biology and chemistry, it allows us to build a complete protein profile, which will be very useful later on.

\subsection{Mapping specific protein sequences and structures to the papers that describe their purification}

To gather the most comprehensive metadata possible, my first priority was to establish a reliable link between PDB entries (3D structures) and their corresponding protein's UniProt entry (proteins). While both databases provide APIs that allow for programmatic queries, developing a custom mapping tool from scratch proved to be more complex than initially anticipated.

The primary challenge was the sheer volume of data. With nearly 250,000 entries in the PDB, each potentially mapping to multiple protein chains and UniProt IDs, the number of required requests was massive. Both the PDB and UniProt APIs enforce strict rate limits to ensure server stability. Considering this, we selected the 200 first entries we get from PDB's API to run our tests on.

In Figure~\ref{fig:old_viz} we have a diagram illustrating our initial setup for the data extraction. We start by fetching PDB entries by querying its API. Most PDB entries link to one or more UniProt IDs, which identify the specific proteins contained in that structure, so we use those IDs to query UniProt's API. With UniProt's data we are able to complete our ID mapping, leaving just the full-text of the paper corresponding to that protein missing. This is primarily obtained through the PubMed Central's ID present in the PDB, which allows us to query Europe PMC's\footnote{\url{https://europepmc.org/}} API for the full-text of the respective paper. Finally, we would need to apply text mining logic to the resulting XMLs to extract the purification processes. This last part was not yet flushed out by this point.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/old_viz}
  \caption{Diagram of old method}
  \label{fig:old_viz}
\end{figure}

My initial tests showed that attempting to complete sync both databases would take an impractical amount of time—potentially days of continuous running—just to establish a baseline mapping. Furthermore, handling the edge cases where one PDB structure corresponds to multiple distinct proteins added significant logic overhead to the scripts.

Recognizing that this manual mapping was becoming a bottleneck, I sought a more efficient alternative. This led me to the European Bioinformatics Institute (EBI) public file server\footnote{\url{https://ftp.ebi.ac.uk/}}. EBI provides precomputed mapping files that are updated weekly, specifically designed to correlate PDB and UniProt entries, along other cross-references. In Figure~\ref{fig:new_viz} we have the update diagram representing this change.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/new_viz}
  \caption{Diagram of new method}
  \label{fig:new_viz}
\end{figure}

Having this simplified mapping allowed us to create better visualizations of the data, such as the distribution of PDB and UniProt IDs that we see in Figure~\ref{fig:mapping_distribution}.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/pdbs_per_uniprot}
    \caption{Number of PDB IDs per UniProt ID}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{diagrams/uniprots_per_pdb}
    \caption{Number of UniProt IDs per PDB ID}
  \end{subfigure}
  \caption{PDB/UniProt ID distribution}
  \label{fig:mapping_distribution}
\end{figure}


\subsection{Automating the retrieval of the complete text of identified papers}

Once the relevant scientific citations were identified and linked to their respective proteins, the next objective was to acquire the full text of these papers. This is a critical step because the specific details of a purification protocol, such as the chromatography steps and their order, are almost exclusively contained within the "Materials and Methods" or "Experimental Procedures" sections of a full manuscript, rather than in the abstract.

After evaluating several biological literature repositories, I concluded that the Europe PMC REST API offered the most robust solution for automated full-text acquisition. Europe PMC is particularly advantageous because it provides a centralized access point for a vast collection of scientific literature and offers a dedicated endpoint for retrieving papers in a machine-readable XML format.

While the PDB provides both DOIs and PMIDs, the Europe PMC API is most efficient when queried using the PMID. Consequently, I implemented a preprocessing step to ensure every entry had a valid PMID. In cases where only a DOI was available, I utilized the NCBI ID Converter API to programmatically resolve the DOI into its corresponding PubMed identifier. With a clean list of PMIDs, the pipeline was then able to systematically request the full-text XML for each record.

After evaluating several biological literature repositories, I concluded that the Europe PMC REST API offered the most robust solution for automated full-text retrieval. Europe PMC is particularly advantageous because it provides a centralized access point for a vast collection of scientific literature and offers a dedicated endpoint for retrieving papers in an easily parsable XML format.

One technical challenge encountered during this phase was the inconsistency of the available metadata. While most entries are complete, a significant number of records lack a PubMed ID or DOI, providing only a publication title. This demanded the development of a flexible retrieval strategy that could fall back on title-based searches when unique digital identifiers were unavailable, ensuring that the maximum amount of relevant literature could be captured for the next stage of the pipeline. The following issues were identified:

\begin{itemize}
  \item \textbf{Repeated PMIDs:} Frequently, multiple PDB entries (representing different structural configurations or mutants of the same protein) reference the exact same primary citation. While not an error, the pipeline had to be optimized to recognize these duplicates to avoid redundant API calls and unnecessary storage use.
  \item \textbf{Not open access:} The most significant hurdle is that not all papers are available in the Europe PMC Open Access subset. Many papers remain behind paywalls, meaning the API can only return the abstract or metadata rather than the complete paper.
  \item \textbf{No citation available:} For some older or more obscure PDB entries, neither a DOI nor a PMID is recorded. Without these unique identifiers, automated retrieval becomes significantly more difficult, often requiring title-based fuzzy matching which is less reliable.
  \item \textbf{Failed to get full text:} In some instances, a record might exist in the database, but the full-text version has not been deposited or processed into the XML format required by our extraction tools.
\end{itemize}

Given the size of the full mapping and the databases we query, using every entry for testing purposes would take a prohibitive amount of time and compute, so a small subset of the first 200 entries was used instead. In Figure~\ref{fig:sankey_result} we see the results for this phase.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/sankey_result}
  \caption{Results Sankey Diagram}
  \label{fig:sankey_result}
\end{figure}

Each arm of the sankey diagram represents a different error received by the data extraction tool, but upon closer inspection the cases where we received an empty result was actually due to lack of open access. Considering this, the main problem was the lack of open access to papers (20\%), followed by the lack of citations in PDB (19.5\%). 

Many PDB entries cite the same primary publication because a single study often reports the experimental determination of multiple protein structures. As a result, the same PMID frequently appears across multiple entries, leading to repeated PMIDs in our dataset (18.5\% of examples). Extracting a single protein purification protocol from unstructured text is already challenging; extracting multiple distinct protocols from the same source and correctly associating each with its corresponding protein will be substantially more difficult. Nevertheless, we anticipate that with the employment of more advanced NLP methods, particularly the use of LLMs, we will be able to achieve a high success rate.

Although repeated PMIDs are challenging to handle, they correspond to successful full-text extractions and, together with the other successful cases, yield an overall success rate of 52.5\%. If this rate were to hold across the more than 249,000 PDB entries, we could, in principle, compile up to approximately 130,000 distinct protein purification protocols, assuming a 100\% success rate in protocol extraction and ignoring potential duplicates.

\subsection{Processing the unstructured text to isolate chromatography steps and related biochemical metadata}

The final and most complex phase of the pipeline involves transforming the retrieved full-text papers into a structured sequence of purification steps. Having the papers in XML format, as provided by the Europe PMC API, proved to be a significant advantage. Unlike PDFs, which are notoriously difficult to parse due to inconsistent layouts, XML documents use standardized tags to identify specific sections, titles, and paragraphs. This structure allowed me to programmatically navigate the document and isolate the most relevant portions of the text.

In the early stages of development, I needed a simple and reliable way to identify where the purification process was described within a paper. After analyzing the structure of around 20 scientific papers, I found that two primary indicators were highly effective:
\begin{enumerate} 
  \item Searching for the keyword "Purification" within subsection titles (e.g., \textit{<title>Protein expression and purification</title>}). 
  \item Identifying paragraphs in the main body that contained the term "chromatography." 
\end{enumerate}

While this approach was successful for locating the general area of interest during preliminary tests, it was not sufficient for our ultimate goal. The specific steps, the tools being used, their sizes, concentrations and other parameters and their order are extremely important to systemically define each purification protocol, so we had to refine our approach.

For this, I leveraged the fact that protein purification is a specialized field with a relatively finite set of techniques. Most protocols follow a logical progression using a limited number of standard methods, such as Affinity Chromatography, Ion Exchange, or Size Exclusion.

By recognizing this pattern, I developed a comprehensive dictionary of chromatography terms, tools, and specific biochemical markers (such as "His-tag," "IMAC," "gradient elution," or "superdex"). The underlying logic is that these technical terms are highly specific; they are rarely mentioned in biological literature outside the context of an actual purification protocol. In Figure~\ref{fig:chrom_techniques} we can see an example of the keywords' hierarchy for the "Size Exclusion" technique category.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/chrom_techniques}
  \caption{Chromatography techniques hierarchy}
  \label{fig:chrom_techniques}
\end{figure}


The current version of the tool scans the identified "Purification" sections and extracts these terms in the order they appear. By capturing this sequence, the pipeline theoretically reconstructs the "recipe" used in the laboratory. For example, if the tool detects "Affinity Chromatography" followed by "Dialysis" and then "Gel Filtration," it records these as three distinct chronological steps.

This methodology is currently a work in progress. While the dictionary-based approach provides a structured way to handle unstructured text, it is not yet perfect. Scientific writing can be nuanced, and the tool must be able to distinguish between a technique that was actually performed and one that is merely being discussed or referenced.

At this stage, I have not yet produced definitive preliminary results from this extraction phase. It remains an iterative process, and I expect to refine the dictionary and the extraction logic as I begin to validate the output against known manual protocols.

% section work_done (end)


\section{Future Work Plan} % (fold)
\label{sec:future_work_plan}

While substantial progress has been made on the data mining pipeline, significant work remains to ensure data quality, develop the predictive model, and validate its performance. The remaining tasks include finalizing the extraction tool, executing it on the complete dataset, defining and training the Transformer architecture, and documenting the findings. Figure~\ref{fig:gantt_chart} presents the proposed timeline for the upcoming semester.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/gantt_chart}
  \caption{Work Plan Chart}
  \label{fig:gantt_chart}
\end{figure}

\subsection{Data Collection and Validation}

Three interdependent subtasks will complete the data mining phase: finalizing the extraction script, validating its output, and executing it on the full dataset. Currently, the pipeline processes only 200 PDB--UniProt entries for testing purposes, as processing the complete dataset during development would be computationally prohibitive.

The overlapping nature of these subtasks reflects the iterative development process inherent to data mining tools. Script refinement proceeds through cycles of execution, error identification, and correction. Validation will involve systematic verification of extracted protocols against source publications and identifying gaps in the extraction logic that require adjustment. Once validation confirms acceptable accuracy, the pipeline will be scaled to the complete dataset, with ongoing validation to address any previously undetected edge cases.


\subsection{Predictive Model Development}

Model development comprises several interconnected subtasks. Initial work will focus on architecture design, involving the evaluation of alternative Transformer configurations based on sequence modeling requirements, attention mechanisms, and the vocabulary of chromatography techniques, as seen in Appendix~\ref{app:appendix}.

The training phase represents the most resource-intensive component of the project. Beyond the computational demands of model training itself, this period encompasses the development of data preprocessing pipelines, implementation of the wrapper algorithm, and iterative refinement based on preliminary results. The wrapper algorithm serves as a gap filling layer that addresses practical limitations of the model, handling tasks such as input normalization, output formatting and constraint enforcement to maximize the generated protocols' usefulness, resulting in a complete end-to-end tool.

\subsection{Results Analysis}

Once the development of the predictive model is mostly finished, we will enter a phase of result analysis and validation. During this phase we will apply our tool to specific case studies and perform a formal qualitative evaluation. Depending on the outcome, some changes may still be applied, but we expect to have a mostly finished tool by this point, so they should be minor. Finally, we will proceed with the publication of the code and model.

% section future_work_plan (end)


% Develop protocol steps predictive pipeline 