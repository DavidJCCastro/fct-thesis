%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%

\chapter{Related Work}
\label{cha:related_work}

\section{Historical context}
\label{sec:a_bit_of_history}

%\ntindex[Historical context]{}

The history of protein purification is intrinsically linked to our understanding of life at the molecular level. This journey began in 1789 when Antoine Fourcroy first distinguished several types of complex organic substances, which he categorized as "albumins," including fibrin, gelatin, and gluten. Although these substances were not yet recognized as proteins, their consistent presence in biological processes made them a primary focus for early chemists. The identification of the building blocks of these substances was a slow process; while asparagine was the first amino acid isolated in 1809, its role as a fundamental constituent of proteins (\ref{fig:protein}) was not fully established until 1873. A critical link was formed earlier, in 1819, with the isolation of leucine, which helped researchers begin to understand the chemical nature of these "albuminous" materials.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/protein}
  \caption{Protein structure}
  \label{fig:protein}
\end{figure}

By 1837, Gerrit J. Mulder determined the elemental composition of several proteins and proposed that they shared a common core substance. In response to these findings, Jacob Berzelius suggested the name "protein" in 1838, derived from the Greek word \textit{proteios}, meaning "primary" or "of the first rank." Despite this naming, the chemical diversity of proteins remained largely unknown; at the time, only glycine and leucine had been identified. It would take nearly another century, until the discovery of threonine in 1936, for the complete set of 20 standard amino acids to be recognized.

A defining moment in the field occurred in 1926, amidst a heated debate over whether enzymes were distinct chemical entities or simply "catalytic forces" associated with proteins. James Sumner settled this by isolating and crystallizing the enzyme urease from jack beans. This achievement provided the first definitive proof that enzymes were proteins with specific, defined chemical structures that could be purified to homogeneity. Sumner's work, which earned him the Nobel Prize in 1946, effectively birthed the field of structural biochemistry and established purification as a prerequisite for understanding protein function.

In the decades following Sumner's breakthrough, the field saw the development of diverse biophysical techniques designed to separate proteins based on their intrinsic properties, such as electrical charge, molecular size, and polarity. These methods—including various forms of chromatography and electrophoresis—became the standard toolkit for biochemists. The landscape of protein science changed again in 1973, when Stanley Cohen and Herbert Boyer developed recombinant DNA technology. This allowed scientists to insert specific DNA sequences into host organisms like \textit{E. coli}, turning bacteria into "factories" for the mass production of specific proteins.

While recombinant technology solved the problem of protein "sourcing," it introduced new challenges for purification. In the 1980s, the development of affinity tags (such as the polyhistidine tag or GST-tag) revolutionized the field by allowing researchers to add a universal "handle" to any recombinant protein. This made purification significantly easier and more predictable. However, these tags can often interfere with the protein's native folding, biological activity, or its suitability for therapeutic use in humans. 

Consequently, the purification of "non-tagged" proteins remains the gold standard for many high-precision applications. Because every non-tagged protein has a unique combination of surface charges and hydrophobic patches, designing an effective purification protocol remains a labor-intensive process of trial and error. This historical difficulty is the primary driver for the current research, as we seek to automate the design of these complex protocols through computational modeling.

\section{Sourcing relevant data}

Because the objective of this work is to train a predictive model, the quality and source of the training data is of upmost importance. Reliable predictions require datasets that accurately reflect successful protein purification outcomes. Considering that, this section outlines the public resources used to assemble the training corpus, including databases providing physico-chemical protein properties and literature sources from which experimentally validated purification protocols can be extracted.

\subsection{Protein Data Bank}

The Protein Data Bank (PDB) is the global repository for three-dimensional structural data of biological macromolecules \cite{Berman2000}. Established in 1971, it serves as a central resource for structural biology by providing open access to validated models of proteins and nucleic acids through their website and an API, which we will be using. Each entry in the PDB represents a successful experiment where a protein was expressed, purified, and its structure determined.

In addition to atomic coordinates, PDB entries contain metadata such as crystallization conditions—including pH and temperature—and references to the primary literature. While the structural data is highly standardized and machine-readable, the specific purification protocols used to obtain these samples are not stored in a structured format within the database. Instead, these procedural details are typically contained within the "Materials and Methods" sections of the cited research papers. As a result, the PDB acts as a link between structured protein data and the unstructured purification processes found in scientific literature.

\subsection{Europe PMC}

Europe PMC serves as a primary open-access repository for life science literature, managed by the European Bioinformatics Institute (EMBL-EBI)\cite{rosonovski2024europe}. At the time of writing, the platform indexes more than 47.5 million abstracts and 11.5 million full-text articles, aggregating data from major sources such as PubMed and PubMed Central. A critical feature relevant to this project is the availability of machine-readable full-text content in XML format, which is specifically designed to support large-scale text and data mining.

Programmatic access is facilitated through RESTful APIs and FTP bulk download services, allowing for the systematic retrieval of research data using standard identifiers like PMIDs or DOIs. Beyond simple document hosting, Europe PMC enriches its corpus with over 2 billion text-mined annotations for biological entities, including proteins, chemicals, and experimental methods, while maintaining reciprocal links to over 60 external life science databases \cite{10.12688/wellcomeopenres.10210.2}.

\subsection{UniProt}

The Universal Protein Resource (UniProt) serves as the primary central repository for protein sequence data and functional annotation. Its core component, the UniProt Knowledgebase (UniProtKB), is structured into two main sections: UniProtKB/Swiss-Prot, which contains high-quality, manually curated entries, and UniProtKB/TrEMBL, which provides computationally annotated sequences \cite{uniprot2025}. 

Beyond simple sequence storage, UniProt facilitates data interoperability by providing a unified identification system with cross-references to other biological databases, such as the Protein Data Bank (PDB) for structural data and PubMed/Europe PMC for primary literature. This integration is essential for my data extraction pipeline, as it allows for the correlation of amino acid sequences with biochemical properties and experimental evidence.

\subsection{Literature mining approaches}
The challenge of extracting structured information from unstructured biomedical literature has been extensively documented in meta-research contexts \cite{beltagy2019scibertpretrainedlanguagemodel, bimed-extract-meta-reasearch, dockes2024mining}. The fundamental scalability problem inherent to manual literature curation is: with over 1 million papers indexed by PubMed annually, traditional manual extraction approaches become prohibitively time-consuming and difficult to reproduce \cite{dockes2024mining}. This limitation is particularly evident when dealing with methodological details buried within specific, unstandardized sections of natural language text, like what we find in the scientific literature.

To address these challenges in their domain, Dockès et al. \cite{dockes2024mining} developed \texttt{pubget}, a command-line tool for bulk downloading and processing articles from PubMed Central, and \texttt{labelbuddy}, a lightweight annotation application for creating ground-truth datasets. While these tools target neuroimaging literature specifically, their underlying approach to automated content extraction and manual validation may prove valuable for protein purification protocol mining, particularly when validation datasets become necessary for training or evaluating other extraction methods.

In Figure~\ref{fig:mining-lit} we see a table with a breakdown of the pros and cons of the different approaches for their case, which is similar to ours. In our case, after researching the possibility of re-using an existing corpus, the ones closest in similarity to what we need were not readily available nor ideal. Manually collecting papers, given its low scalability, was also not suited for our needs, considering we needed a very large amount of data for our purpose. This left us no choice but to automatically collecting papers, which is what our data mining tool attempts to do.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/mining-lit}
  \caption{Literature mining approaches \cite{dockes2024mining}}
  \label{fig:mining-lit}
\end{figure}

The authors' comparison of extraction methodologies provides important context for rule-based approaches like the keyword hierarchy employed in this work. When extracting participant demographics from neuroimaging studies, their heuristic method achieved exact matches in only 36\% of cases, compared to 50\% for zero-shot GPT-3.5 prompting \cite{dockes2024mining}. However, the heuristic approach demonstrated comparable accuracy when it did make predictions (0\% median absolute percent error for both methods), with the primary difference being in recall—GPT-3.5 made predictions for 100\% of papers versus 54\% for the rule-based system. These results suggest that dictionary-based extraction, while potentially limited in coverage, can achieve acceptable precision for domain-specific information retrieval when patterns are sufficiently regular.

For the current phase of this project, a keyword-based approach offers the advantages of transparency, reproducibility, and computational efficiency. However, should initial results indicate insufficient recall or accuracy in capturing the full diversity of purification protocols, the precedent set by Dockès et al. demonstrates that large language models represent a viable alternative extraction strategy worth investigating.

\section{Transformer-based models for text mining}

The extraction of structured information from biomedical literature has traditionally relied on rule-based or dictionary-based approaches. While these methods can achieve high precision in identifying specific chromatography techniques or experimental parameters within defined contexts, they often struggle with the inherent linguistic variability and complex terminology found in scientific text \cite{lee2020biobert}. Rule-based systems are frequently limited by the requirement for exhaustive keyword hierarchies and their inability to capture the broader semantic context of a sentence. To address these limitations, recent advancements in Natural Language Processing (NLP) have shifted toward deep learning architectures, most notably the Transformer.

\subsection{BioBERT}
The Bidirectional Encoder Representations from Transformers (BERT) model introduced contextualized word representations, allowing for a more nuanced understanding of text \cite{devlin-etal-2019-bert}. However, general-domain models often perform poorly on specialized scientific literature due to the significant shift in word distribution between general corpora and biomedical text \cite{lee2020biobert}.

BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) addresses this domain gap by initializing with general-purpose BERT weights and undergoing further pre-training on large-scale biomedical corpora, specifically PubMed abstracts and PubMed Central full-text articles \cite{lee2020biobert}. This domain-specific pre-training enables the model to effectively recognize complex biomedical entities and relationships that rule-based systems might miss. By leveraging BioBERT's capabilities, it is possible to automate the extraction of relevant purification keywords and chromatography steps with significantly higher accuracy than traditional methods. This would be achieved by identifying each purification step through Named Entity Recognition (NER) and correctly structuring them through Relation Extraction (RE).

\subsection{GLiNER-BioMed}

While models like BioBERT have significantly improved biomedical NER, they remain constrained by a fixed taxonomy, requiring the fine-tuning of a classification head for a pre-defined set of entities \cite{lee2020biobert, yazdani2025gliner}. This limitation makes them less adaptable to more specific domains, where certain entity types have no training data, as is our case of chromatography techniques. To overcome these challenges, GLiNER-BioMed introduces an "open NER" framework that treats entity recognition as a matching problem between text spans and natural language labels \cite{yazdani2025gliner}.

The primary advantage of GLiNER-BioMed for automated data mining lies in its ability to perform zero-shot recognition, enabling the extraction of arbitrary entity types without model retraining. This flexibility is achieved through a domain-specific adaptation of the Generalist and Lightweight Model for NER (GLiNER) \cite{yazdani2025gliner}. The development of GLiNER-BioMed involved several key techniques, most notably synthetic data distillation. In this process, a large-scale teacher model (OpenBioLLM-70B) was used to generate high-quality NER annotations, which were then used to train a smaller student model to efficiently annotate a 105,000-sample pre-training corpus \cite{yazdani2025gliner}. We can see a diagram of this workflow in Figure~\ref{fig:gliner_biomed_dataset}. This particular technique might prove useful if we come to train our own purification protocol extractor.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/gliner_biomed_dataset}
  \caption{Overview of the synthetic pre-training data generation pipeline \cite{yazdani2025gliner}}
  \label{fig:gliner_biomed_dataset}
\end{figure}

\subsection{Large-scale relation extraction and knowledge integration}

Building upon entity recognition, the final stage of automated data mining involves identifying the complex relationships between extracted entities to reconstruct sequential protocols. While BioBERT and GLiNER-BioMed excel at isolating specific technical terms, the work of Zhang et al. (2023) demonstrates how transformer-based models can be optimized for large-scale biomedical relation extraction (RE) across diverse categories \cite{zhang2023large}. Their research highlights several techniques that are particularly relevant for transforming raw extraction outputs into structured "laboratory-ready" recipes.

A key finding from their study is that the performance of relation extraction is significantly enhanced when entities are enriched with detailed semantic information. By incorporating semantic type names into the model's input representation, the architecture can better understand the functional role of an entity, thereby improving the accuracy of predicted relationships \cite{zhang2023large}. In our work this technique could be replicated using our chromatography techniques dictionary, where we would instead enrich our entities with the hierarchical information of the purification technique, for example.

For the objective of generating purification protocols, the integration of these extracted relations into a Knowledge Graph (KG) framework, as implemented by Zhang et al. (2023), offers a robust method for storing and querying chronological sequences \cite{zhang2023large}. By representing purification steps as nodes and their sequential connections as edges, it becomes possible to treat protocol generation as a structured sequence discovery problem.


\section{Protein representation}
In the context of predicting laboratory protocols, the representation of a protein must encapsulate more than its sequence to account for the diverse physico-chemical behaviors encountered during chromatography. Recent developments in protein representation learning have focused on addressing the inherent "knowledge gap" in standard protein language models (LMs), which often fail to capture factual biological context. A notable contribution is the Knowledge-exploited Auto-encoder for Protein (KeAP), which introduces a more granular, token-level exploration of knowledge graphs to enrich primary structure modeling \cite{bian2023keap}.

Unlike earlier models that integrated knowledge at a coarse sequence-wide level, KeAP utilizes a cross-attention mechanism where individual amino acids iteratively query associated knowledge tokens, specifically relation and attribute terms derived from Gene Ontology (GO). This interaction allows the model to integrate functional and structural descriptors, such as molecular functions and cellular localizations, directly into the protein representation. This granular integration enables the production of a "better-contextualized" representation that has shown superior performance in predicting downstream properties like protein stability and binding affinity. For a system designed to generate purification recipes, this paradigm is highly applicable, as it allows the input representation to explicitly leverage biological metadata that dictate chromatography behavior, providing a richer foundation for purification protocol prediction.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/keap}
  \caption{KeAP's cross attention mechanism}
  \label{fig:keap}
\end{figure}

In Figure~\ref{fig:keap} we have a diagram of KeAP's cross attention mechanism from the original paper. Given a knowledge-graph triplet (Protein, Relation, Attribute), the protein sequence is encoded into amino-acid embeddings, while the associated relation and attribute texts are encoded into word-level representations. Knowledge integration is performed in the protein decoder through stacked Protein--Knowledge exploration (PiK) blocks.


\section{Similar Work}

% protein purification protocol predictive model (4-PM)

% protein-purification-protocol-predictive model (4-PM) (4P-M)

Previous work has addressed problems similar to those considered here. Training a model to predict protein purification protocols requires a large amount of high-quality data, which motivates the development of our data mining tool. Although such work would ideally rely on an existing database linking proteins to their purification steps, we were unable to find any accessible database that provides this information.

\subsection{PurificationDB}

PurificationDB is a curated knowledge base specifically designed to aggregate experimentally validated protein purification conditions from the literature \cite{garland2023purificationdb}. The database was constructed by first identifying protein structures deposited in the PDB, under the assumption that successful structure determination implies prior successful purification. Associated publications were retrieved via PDB-linked DOIs, and full texts were collected primarily from crystallography reports. From these documents, purification-related information was extracted using a rule-based named-entity recognition (NER) framework informed by expert-defined vocabularies covering chromatography techniques, buffer components, and concentration units.

From the perspective of our work, PurificationDB represents a highly relevant prior effort, as it targets the systematic extraction and structuring of purification knowledge from unstructured literature, closely aligning with our initial objective of compiling a protein purification protocol database. However, despite the authors of the paper providing a URL\footnote{\url{https://purificationdatabase.herokuapp.com/}} where we should be able to access the "open-access and user-friendly knowledge base", it was not accessible at the time of our investigation. This limitation further motivates the development of reproducible, fully automated data mining pipeline that operates on publicly available repositories and does not depend on the long-term availability of third-party curated databases.

\subsection{Extraction of purification protocol information using a LLM}

A closely related study by Chen and Sivaraman~\cite{chen2025llmpurification} demonstrates the feasibility of leveraging large language models (LLMs) to systematically extract protein expression and purification strategies from scientific papers indexed in the PDB. Similar to our work, their central premise is that proteins with solved three-dimensional structures represent successful prior purification efforts, and thus constitute a high-quality empirical foundation for learning purification strategies.

Methodologically, their pipeline parallels our data mining framework in several ways. First, both approaches use the PDB as an entry point to anchor experimental protocols to validated protein sequences via UniProt identifiers. Second, the authors perform large-scale full-text processing of structural biology articles, with a strong focus on sections where purification details are reported. Third, both studies aim to convert unstructured experimental descriptions into structured, machine-interpretable representations of purification procedures.

A notable contribution from this paper is their hybrid information extraction strategy, which combines dense text embeddings with a multi-step LLM prompting scheme to localize and extract protocol-relevant passages. Specifically, they employ embedding-based section ranking to restrict LLM attention to purification-related text segments, followed by a two-step LLM extraction process and structured prompts to reduce hallucination and misclassification errors. These design choices are directly relevant to our work, as they provide validated techniques for improving the precision of protocol reconstruction from long and unstructured biomedical articles. In Figure~\ref{fig:llm-pro} we see a diagram representing this strategy.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/llm-pro}
  \caption{Workflow of the Efficient Article Information Extraction Tool}
  \label{fig:llm-pro}
\end{figure}

Although the goals of this paper are closely aligned with those of our data extraction tool, its contributions are purely theoretical, as the authors do not make any of their developed resources publicly available. While they state that the data supporting their findings can be obtained from the corresponding author upon reasonable request, access would be restricted to the specific dataset extracted at the time of their study, thereby limiting the scope of data we would have to work with.

Furthermore, our project uses the extracted data for a fundamentally different purpose---namely, the training of a predictive model---, whereas the aforementioned study employs it for a statistical analysis of purification strategies. In addition, we aim to construct a larger database that incorporates other protein-related information to be used as input features for prediction and model training. We emphasize this distinction because different objectives can lead to subtle but significant differences in the approaches used for data extraction and preprocessing.