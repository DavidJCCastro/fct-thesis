%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter2.tex
%% NOVA thesis document file
%%
%% Chapter with the template manual
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter2.tex}%

\chapter{Related Work}
\label{cha:related_work}

\section{Sourcing relevant data}

Because the objective of this work is to train a predictive model, the quality and source of the training data is of upmost importance. Reliable predictions require datasets that accurately reflect successful protein purification outcomes. Considering that, this section outlines the public resources used to assemble the training corpus, including databases providing physico-chemical protein properties and literature sources from which experimentally validated purification protocols can be extracted.

\subsection{Protein Data Bank}

The Protein Data Bank (PDB) is the global repository for three-dimensional structural data of biological macromolecules \cite{Berman2000}. Established in 1971, it serves as a central resource for structural biology by providing open access to validated models of proteins and nucleic acids through their website and an API, which we will be using. Each entry in the PDB represents a successful experiment where a protein was expressed, purified, and its structure determined.

In addition to atomic coordinates, PDB entries contain metadata such as crystallization conditions—including pH and temperature—and references to the primary literature \cite{Berman2000}. While the structural data is highly standardized and machine-readable, the specific purification protocols used to obtain these samples are not stored in a structured format within the database. Instead, these procedural details are typically contained within the "Materials and Methods" sections of the cited research papers. As a result, the PDB acts as a link between structured protein data and the unstructured purification processes found in scientific literature.

\subsection{Europe PMC}

Europe PMC serves as a primary open-access repository for life science literature, managed by the European Bioinformatics Institute (EMBL-EBI)\cite{rosonovski2024europe}. As of late 2023, the platform indexes more than 42 million abstracts and 9 million full-text articles, aggregating data from major sources such as PubMed and PubMed Central \cite{rosonovski2024europe}. A critical feature relevant to this project is the availability of machine-readable full-text content in XML format, which is specifically designed to support large-scale text and data mining.

Programmatic access is facilitated through RESTful APIs and FTP bulk download services, allowing for the systematic retrieval of research data using standard identifiers like PMIDs or DOIs \cite{rosonovski2024europe}. Beyond simple document hosting, Europe PMC enriches its corpus with over 2 billion text-mined annotations for biological entities, including proteins, chemicals, and experimental methods, while maintaining reciprocal links to over 60 external life science databases \cite{rosonovski2024europe}.

\subsection{UniProt}

The Universal Protein Resource (UniProt) serves as the primary central repository for protein sequence data and functional annotation. Its core component, the UniProt Knowledgebase (UniProtKB), is structured into two main sections: UniProtKB/Swiss-Prot, which contains high-quality, manually curated entries, and UniProtKB/TrEMBL, which provides computationally annotated sequences \cite{uniprot2025}. 

Beyond simple sequence storage, UniProt facilitates data interoperability by providing a unified identification system with cross-references to other biological databases, such as the Protein Data Bank (PDB) for structural data and PubMed/Europe PMC for primary literature. This integration is essential for my data extraction pipeline, as it allows for the correlation of primary amino acid sequences with biochemical properties and experimental evidence.

\subsection{Literature Mining Approaches}
The challenge of extracting structured information from unstructured biomedical literature has been extensively documented in meta-research contexts. The fundamental scalability problem inherent to manual literature curation is: with over 1 million papers indexed by PubMed annually, traditional manual extraction approaches become prohibitively time-consuming and difficult to reproduce\cite{dockes2024mining}. This limitation is particularly evident when dealing with methodological details buried within specific, unstandardized sections of natural language text, like what we find in the scientific literature.

To address these challenges in their domain, Dockès et al. developed \texttt{pubget}, a command-line tool for bulk downloading and processing articles from PubMed Central, and \texttt{labelbuddy}, a lightweight annotation application for creating ground-truth datasets. While these tools target neuroimaging literature specifically, their underlying approach to automated content extraction and manual validation may prove valuable for future iterations of protein purification protocol mining, particularly when validation datasets become necessary for training or evaluating other extraction methods. In figure~\ref{fig:mining-lit} we see a table with a breakdown of the pros and cons of the different approaches for their case, which is similar to ours.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/mining-lit}
  \caption{Literature mining approaches}
  \label{fig:mining-lit}
\end{figure}

The authors' comparison of extraction methodologies provides important context for rule-based approaches like the keyword hierarchy employed in this work. When extracting participant demographics from neuroimaging studies, their heuristic method achieved exact matches in only 36\% of cases, compared to 50\% for zero-shot GPT-3.5 prompting \cite{dockes2024mining}. However, the heuristic approach demonstrated comparable accuracy when it did make predictions (0\% median absolute percent error for both methods), with the primary difference being in recall—GPT-3.5 made predictions for 100\% of papers versus 54\% for the rule-based system. These results suggest that dictionary-based extraction, while potentially limited in coverage, can achieve acceptable precision for domain-specific information retrieval when patterns are sufficiently regular.

For the current phase of this project, a keyword-based approach offers the advantages of transparency, reproducibility, and computational efficiency. However, should initial results indicate insufficient recall or accuracy in capturing the full diversity of purification protocols, the precedent set by Dockès et al. demonstrates that large language models represent a viable alternative extraction strategy worth investigating.

\section{Transformer-based models for text mining}

The extraction of structured information from biomedical literature has traditionally relied on rule-based or dictionary-based approaches. While these methods can achieve high precision in identifying specific chromatography techniques or experimental parameters within defined contexts, they often struggle with the inherent linguistic variability and complex terminology found in scientific text \cite{lee2020biobert}. Rule-based systems are frequently limited by the requirement for exhaustive keyword hierarchies and their inability to capture the broader semantic context of a sentence.

\subsection{BioBERT}
To address these limitations, recent advancements in Natural Language Processing (NLP) have shifted toward deep learning architectures, most notably the Transformer. The Bidirectional Encoder Representations from Transformers (BERT) model introduced contextualized word representations, allowing for a more nuanced understanding of text \cite{lee2020biobert}. However, general-domain models often perform poorly on specialized scientific literature due to the significant shift in word distribution between general corpora and biomedical text \cite{lee2020biobert}.

BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) addresses this domain gap by initializing with general-purpose BERT weights and undergoing further pre-training on large-scale biomedical corpora, specifically PubMed abstracts and PubMed Central full-text articles \cite{lee2020biobert}. This domain-specific pre-training enables the model to effectively recognize complex biomedical entities and relationships that rule-based systems might miss. By leveraging BioBERT for tasks such as Named Entity Recognition (NER), it is possible to automate the extraction of relevant purification keywords and chromatography steps with significantly higher accuracy than traditional methods, providing a robust foundation for sequential protocol generation \cite{lee2020biobert}.

\subsection{GLiNER-BioMed}

While models like BioBERT have significantly improved biomedical named entity recognition (NER), they remain constrained by a fixed taxonomy, requiring the fine-tuning of a classification head for a pre-defined set of entities \cite{lee2020biobert, yazdani2025gliner}. This limitation makes them less adaptable to the evolving or highly specific terminology encountered in protein purification protocols, where new chromatography resins or buffer additives may not be present in the training set \cite{yazdani2025gliner}. To overcome these challenges, GLiNER-BioMed introduces an "open NER" framework that treats entity recognition as a matching problem between text spans and natural language labels \cite{yazdani2025gliner}.

The primary advantage of GLiNER-BioMed for automated data mining lies in its ability to perform zero-shot recognition, enabling the extraction of arbitrary entity types without model retraining \cite{yazdani2025gliner}. This flexibility is achieved through a domain-specific adaptation of the Generalist and Lightweight Model for NER (GLiNER). The development of GLiNER-BioMed involved several key techniques, most notably synthetic data distillation. In this process, a large-scale teacher model (OpenBioLLM-70B) was used to generate high-quality NER annotations, which were then used to train a smaller student model to efficiently annotate a 105,000-sample pre-training corpus \cite{yazdani2025gliner}. We can see a diagram of this workflow in figure~\ref{fig:gliner_biomed_dataset}. This particular technique might prove useful if we come to train our own purification protocol extractor.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/gliner_biomed_dataset}
  \caption{Overview of the synthetic pre-training data generation pipeline}
  \label{fig:gliner_biomed_dataset}
\end{figure}

\subsection{Large-Scale Relation Extraction and Knowledge Integration}

Building upon entity recognition, the final stage of automated data mining involves identifying the complex relationships between extracted entities to reconstruct sequential protocols. While BioBERT and GLiNER-BioMed excel at isolating specific technical terms, the work of Zhang et al. (2023) demonstrates how transformer-based models can be optimized for large-scale biomedical relation extraction (RE) across diverse categories \cite{lee2020biobert, yazdani2025gliner, zhang2023large}. Their research highlights several techniques that are particularly relevant for transforming raw extraction outputs into structured "laboratory-ready" recipes.

A key finding from their study is that the performance of relation extraction is significantly enhanced when entities are enriched with detailed semantic information\cite{zhang2023large}. By incorporating semantic type names into the model's input representation, the architecture can better understand the functional role of an entity, thereby improving the accuracy of predicted relationships \cite{zhang2023large}. In our work this technique could be replicated using our chromatography techniques dictionary, where we would instead enrich our entities with the hierarchical information of the purification technique, for example.

For the objective of generating purification protocols, the integration of these extracted relations into a Knowledge Graph (KG) framework, as implemented by Zhang et al. (2023), offers a robust method for storing and querying chronological sequences \cite{zhang2023large}. By representing purification steps as nodes and their sequential connections as edges, it becomes possible to treat protocol generation as a structured sequence discovery problem.


\section{Protein Representation}
In the context of predicting laboratory protocols, the representation of a protein must encapsulate more than its primary sequence to account for the diverse physico-chemical behaviors encountered during chromatography. Recent developments in protein representation learning have focused on addressing the inherent "knowledge gap" in standard protein language models (LMs), which often fail to capture factual biological context \cite{bian2023keap}. A notable contribution is the Knowledge-exploited Auto-encoder for Protein (KeAP), which introduces a more granular, token-level exploration of knowledge graphs to enrich primary structure modeling \cite{bian2023keap}.

Unlike earlier models that integrated knowledge at a coarse sequence-wide level, KeAP utilizes a cross-attention mechanism where individual amino acids iteratively query associated knowledge tokens, specifically relation and attribute terms derived from Gene Ontology (GO) \cite{bian2023keap}. This interaction allows the model to integrate functional and structural descriptors, such as molecular functions and cellular localizations, directly into the protein representation \cite{bian2023keap}. This granular integration enables the production of a "better-contextualized" representation that has shown superior performance in predicting downstream properties like protein stability and binding affinity \cite{bian2023keap}. For a system designed to generate purification recipes, this paradigm is highly applicable, as it allows the input representation to explicitly leverage biological metadata that dictate chromatography behavior, providing a richer foundation for sequential instruction generation.

\subsection{Integrating Curated Purification Knowledge}
While models like KeAP enhance protein representations through general biological context, the practical application of these representations to protocol generation requires domain-specific "ground truth" data. \textit{PurificationDB} addresses this by providing a curated, standardized database of 4,732 purification entries extracted from literature linked to the Protein Data Bank (PDB) \cite{garland2023purificationdb}. 

The methodology behind PurificationDB directly informs the development of automated prediction pipelines. Notably, the database utilizes named-entity recognition (NER) supported by expert-curated reference tables that include manufacturer-specific nomenclature and chemical synonyms, which can be used to refine keyword-based extraction hierarchies \cite{garland2023purificationdb}. Furthermore, PurificationDB records the sentence index of chromatography steps to preserve the chronological order of the protocol, providing the sequential labels necessary for training Transformer-based architectures \cite{garland2023purificationdb}. 

By combining the token-level representation techniques of KeAP with the structured laboratory conditions found in PurificationDB, it is possible to bridge the gap between primary sequence data and laboratory-ready recipes. In this context, the curated attributes from PurificationDB—such as optimal pH and salt concentrations—can serve as "purification knowledge tokens," similar to the GO-based tokens used in KeAP, to provide a more targeted representation for sequential instruction generation.

\section{Similar Work}

A closely related study by Chen and Sivaraman~\cite{chen2025llmpurification} demonstrates the feasibility of leveraging large language models (LLMs) to systematically extract protein expression and purification strategies from scientific papers indexed in the PDB. Similar to our work, their central premise is that proteins with solved three-dimensional structures represent successful prior purification efforts, and thus constitute a high-quality empirical foundation for learning purification strategies.

Methodologically, their pipeline parallels our data mining framework in several ways. First, both approaches use the PDB as an entry point to anchor experimental protocols to validated protein sequences via UniProt identifiers. Second, the authors perform large-scale full-text processing of structural biology articles, with a strong focus on sections where purification details are reported. Third, both studies aim to convert unstructured experimental descriptions into structured, machine-interpretable representations of purification procedures.

A notable contribution from this paper is their hybrid information extraction strategy, which combines dense text embeddings with a multi-step LLM prompting scheme to localize and extract protocol-relevant passages. Specifically, they employ embedding-based section ranking to restrict LLM attention to purification-related text segments, followed by a two-step LLM extraction process and structured prompts to reduce hallucination and misclassification errors. These design choices are directly relevant to our work, as they provide validated techniques for improving the precision of protocol reconstruction from long and unstructured biomedical articles. In figure~\ref{fig:llm-pro} we see a diagram representing this strategy.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{diagrams/llm-pro}
  \caption{Workflow of the Efficient Article Information Extraction Tool}
  \label{fig:llm-pro}
\end{figure}